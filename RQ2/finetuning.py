import os
import datasets
import transformers
import numpy as np
import pandas as pd
from datasets import load_dataset, load_metric, Dataset

# setting parameters
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
seed = 42
model_name = "codet5"
pretrainedmodel_path = "E:/models/codet5-base"  

labels_num = 4  # number of labels for the classification task
train_epochs = 20
learning_rate = 5e-5
dataset_path = "dataset.json"

# load the dataset and make statistics
severity_dataset = pd.read_json(dataset_path)
labels_distr=pd.value_counts(severity_dataset.labels)
proportion = pd.value_counts(severity_dataset.labels, normalize=True)
print(len(severity_dataset))
print(labels_distr)
print(proportion)


# processing and splitting the dataset
dataset = Dataset.from_dict(severity_dataset)
dataset = dataset.remove_columns(['cweId'])
traintest = dataset.train_test_split(test_size=0.2, seed=seed)
validationtest = traintest['test'].train_test_split(test_size=0.5, seed=seed)
dataset_train = traintest['train']
dataset_val = validationtest['train']
dataset_test = validationtest['test']



# tokenize
from transformers import RobertaTokenizer,AutoConfig
tokenizer = RobertaTokenizer.from_pretrained(pretrainedmodel_path)

def preprocess_function(examples):
    return tokenizer(examples["description"], truncation=True)

encoded_dataset_train = dataset_train.map(preprocess_function, batched=True)
encoded_dataset_val = dataset_val.map(preprocess_function, batched=True)
encoded_dataset_test = dataset_test.map(preprocess_function, batched=True)
encoded_dataset_train.set_format("torch")
encoded_dataset_val.set_format("torch")
encoded_dataset_test.set_format("torch")


from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# load PLM for classification
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, T5Model
model = T5Model.from_pretrained(pretrainedmodel_path, num_labels=labels_num)




# define metrics
metric = load_metric("f1")
metric_name = "f1"
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    f1 = metric.compute(predictions=predictions, references=labels, average='weighted')
    return f1

def compute_metrics2(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    weighted_f1 = metric.compute(predictions=predictions, references=labels, average='weighted')
    micro_f1 = metric.compute(predictions=predictions, references=labels, average='micro')
    macro_f1 = metric.compute(predictions=predictions, references=labels, average='macro')
    return {
        'weighted_f1': weighted_f1,
        'micro_f1': micro_f1,
        'macro_f1': macro_f1,
    }



# train
args = TrainingArguments(
    f"{model_name}-finetuning",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=learning_rate,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    num_train_epochs=train_epochs,
    weight_decay=0.01,
    load_best_model_at_end=True,
    # metric_for_best_model=metric_name,
)
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset_train,
    eval_dataset=encoded_dataset_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    #compute_metrics=compute_metrics
)

trainer.train()

# test
tester = Trainer(
    model,
    args,
    train_dataset=encoded_dataset_train,
    eval_dataset=encoded_dataset_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    #compute_metrics=compute_metrics2
)
tester.evaluate()


